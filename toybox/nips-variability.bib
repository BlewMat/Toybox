@article{wang2016sample,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous Methods for Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning (ICML)},
  pages={1928--1937},
  year={2016}
}

@inproceedings{Mnih2016,
 author = {Mnih, Volodymyr and Badia, Adri\`{a} Puigdom\`{e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
 title = {Asynchronous Methods for Deep Reinforcement Learning},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 series = {ICML'16},
 year = {2016},
 location = {New York, NY, USA},
 pages = {1928--1937},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045390.3045594},
 acmid = {3045594},
 publisher = {JMLR.org},
} 

@incollection{NIPS2017_7112,
title = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5279--5288},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7112-scalable-trust-region-method-for-deep-reinforcement-learning-using-kronecker-factored-approximation.pdf}
}

@inproceedings{cohen2018distributed,
author = {Cohen, Daniel and Jordan, Scott M. and Croft, W. Bruce},
title = {Distributed Evaluations: Ending Neural Point Metrics},
year = {2018},
booktitle = {SIGIR; LND4IR},
type = {IR},
}

@inproceedings{islam2017reproducibility,
    title={{Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control}},
    author={Riashat Islam and Peter Henderson and Maziar Gomrokchi and Doina Precup},
    year={2017},
    booktitle={{Reproducibility in Machine Learning Workshop (ICML)}},
    publisher={arXiv preprint:1708.04133}
}

@inproceedings{henderson2017deep,
  title={{Deep Reinforcement Learning that Matters}},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={{AAAI Conference on Artificial Intelligence (AAAI)}},
publisher={arXiv preprint 1709.06560},
  year={2017}
}

@article{Jensen2000,
author="Jensen, David D.
and Cohen, Paul R.",
title={{Multiple Comparisons in Induction Algorithms}},
journal="Machine Learning",
year="2000",
month="Mar",
day="01",
volume="38",
number="3",
pages="309--338",
abstract="A single mechanism is responsible for three pathologies of induction algorithms: attribute selection errors, overfitting, and oversearching. In each pathology, induction algorithms compare multiple items based on scores from an evaluation function and select the item with the maximum score. We call this a multiple comparison procedure (MCP). We analyze the statistical properties of MCPs and show how failure to adjust for these properties leads to the pathologies. We also discuss approaches that can control pathological behavior, including Bonferroni adjustment, randomization testing, and cross-validation.",
issn="1573-0565",
doi="10.1023/A:1007631014630",
url="https://doi.org/10.1023/A:1007631014630"
}

@article{gelman2013garden,
  title={{The Garden of Forking Paths: Why multiple comparisons can be a problem, even when there is no ``fishing expedition'' or ``p-hacking'' and the research hypothesis was posited ahead of time}},
  author={Gelman, Andrew and Loken, Eric},
  year={2013}
}

@misc{OpenAI_ppo,
  author = {{OpenAI}},
  title = {{Proximal Policy Optimization}},
  howpublished = {\url{https://blog.openai.com/openai-baselines-ppo/}},
  year = {2017}}

@misc{rlblogpost,
    title={{Deep Reinforcement Learning Doesn't Work Yet}},
    author={Irpan, Alex},
    howpublished={\url{https://www.alexirpan.com/2018/02/14/rl-hard.html}},
    year={2018}
}


@article{hwangbo2017control,
  title={{Control of a Quadrotor with Reinforcement Learning}},
  author={Hwangbo, Jemin and Sa, Inkyu and Siegwart, Roland and Hutter, Marco},
  journal={IEEE Robotics and Automation Letters},
  volume={2},
  number={4},
  pages={2096--2103},
  year={2017},
  publisher={IEEE}
}

@inproceedings{wu2017scalable,
  title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle={Advances in neural information processing systems},
  pages={5279--5288},
  year={2017}
}

@article{schulman2017proximal,
  title={{Proximal Policy Optimization Algorithms}},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{todorov2012mujoco,
  title={{Mujoco: A Physics Engine for Model-based Control}},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{arulkumaran2017brief,
  title={{A Brief Survey of Deep Reinforcement Learning}},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  year={1998},
  publisher={MIT press}
}

@article{zhang2018study,
  title={A Study on Overfitting in Deep Reinforcement Learning},
  author={Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  journal={arXiv preprint arXiv:1804.06893},
  year={2018}
}

@inproceedings{whiteson2011protecting,
  title={{Protecting against Evaluation Overfitting in Empirical Reinforcement Learning}},
  author={Whiteson, Shimon and Tanner, Brian and Taylor, Matthew E and Stone, Peter},
  booktitle={Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011 IEEE Symposium on},
  pages={120--127},
  year={2011},
  organization={IEEE}
}

@article{pohlen2018observe,
  title={Observe and Look Further: Achieving Consistent Performance on Atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'\i}k, Mel and others},
  journal={arXiv preprint arXiv:1805.11593},
  year={2018}
}

@article{mnih2013playing,
  title={{Playing Atari with Deep Reinforcement Learning}},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
@article{mnih2015nature,
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	Date = {2015/02/25/online},
	Date-Added = {2018-10-15 17:20:18 +0000},
	Date-Modified = {2018-10-15 17:20:42 +0000},
	Day = {25},
	Journal = {Nature},
	L3 = {10.1038/nature14236; https://www.nature.com/articles/nature14236#supplementary-information},
	Month = {Feb.},
	Pages = {518--529},
	Publisher = {Nature Publishing Group.},
	Title = {{Human-level Control through Deep Reinforcement Learning}},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nature14236},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nature14236}}
% open-AI-gym
@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {{OpenAI Gym}},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
% open-AI-gym-baselines
@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {{OpenAI Baselines}},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@Article{bellemare13arcade,
  author = {{Bellemare}, M.~G. and {Naddaf}, Y. and {Veness}, J. and {Bowling}, M.},
  title = {{The Arcade Learning Environment: An Evaluation Platform for General Agents}},
  journal = {Journal of Artificial Intelligence Research},
  year = "2013",
  month = "Jun",
  volume = "47",
  pages = "253--279",
}
@Article{machado17arcade,
  author = {Marlos C. Machado and Marc G. Bellemare and Erik Talvitie and Joel Veness and Matthew J. Hausknecht and Michael Bowling},
  title = {{Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents}},
  journal = {CoRR},
  volume = {abs/1709.06009},
  year = {2017}
}
